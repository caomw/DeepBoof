- Backwards Functions
  * Drop Out

- Optimization
  * Momentum
  * Nesterov Momentum
  * Adam

- Train a sequential network
- Train and Run LSTM

- Does batch normalization need to obey the training flag?